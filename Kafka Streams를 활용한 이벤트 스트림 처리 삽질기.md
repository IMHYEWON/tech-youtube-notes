## Kafka Streams를 활용한 이벤트 스트림 처리 삽질기 (#우아콘 2023)
### 세션의 목표
배치 만능주의 탈피, 카프카 스트림 도입에 대한 막연한 두려움 완화, 스트림을 도입했을 때 발생할 수 있는 장애 포인트 사전 인지

1. [스트림 처리 도입 배경](##1.스트림-처리-도입-배경)
2. [도메인 요구사항과 전략](#두-번째-항목)
3. [Trouble Shooting - 이슈와 해결방안](##3.Trouble-Shooting-이슈와-해결방안)
4. [모니터링과 관리](##-4.모니터링과-관리)

----

## 1. 스트림 처리 도입 배경
배달을 수행하려면..?   
배민 라이더(배달 수행), 배민 라이더의 위치, 시간과 비용, 기상현상, 도로 환경 등의 데이터가 필요

`배달 인프라 상황`
- 외부데이터 : 기상정보, 교통정보
- 내부데이터 : **지역별 배달, 라이더 현황 등**
- ⇒ 배달 인프라 지표, 배달 인프라 레벨

### 스트림 처리로 전환해야 했던 이유 
#### ASIS : 배치 처리
이벤트 -> 별도의 DB -> 배치 스케쥴러 실행 -> 데이터 가져오기 -> 가공 및 별도의 DB 적재 -> 사용
문제점   
- 일 수백만건 주문 처리를 위한 대규모 파생 데이터 발생
- 요구사항 증가로 인해 점점 복잡해지는 처리 로직
- 주문 수 편차로 인해 예측이 어려운 처리 시간
- 배치 주기에 따라 자구 한발 느린 이상 탐지 (실시간 인지가 어려움)
→ 뷰 모델, 마이크로 배치 등 여러 전략을 사용해보았으나 근본적인 한계에 부딪침    
⇒ 대량 데이터를 실시간으로 처리해야할 필요성, 주기적으로 모아서 처리하는 방식은 도메인에 적합하지 않다고 판단함.

#### Apache Flink가 아닌 Kafka Streams를 선택한 이유?
||Apache Flink|Kafka Streams|
|------|---|---|
|특징|독립적인 클러스터 프레임워크|표준 자바 애플리케이션에 포함 가능한 라이브러리|
|배포 및 실행|Flink 클러스터에 배포하여 별도 작업 실행|**별도 클러스터 구성 없이 애플리케이션 내에서 실행**|
|일반적인 담당 팀|데이터 인프라, BI|**비즈니스 애플리케이션 담당 팀**|
|지원하는 서드 파티|카프카, 카산드라, 다이나모DB 등|카프카의 매커니즘 사용|
|학습 난이도|잘 작성된 문서와 다양한 사례로 인해 쉬움|직관적이지 않은 문서와 국내 사례 부족으로 인한 어려움|


#### Kafka Streams의 기본 개념
- Kafka 매커니즘을 활용한 구조 : 토픽, 파티션, 컨슈머 그룹 등의 구성요소를 그대로 활용
  - 스트림 프로세서 = 카프카 컨슈머
  - 스트림 프로세서의 상태 저장소 = 로컬 저장소를 활용하며, 스트림 프로세서로 들어오는 데이터만을 관리하기 때문에 분산저장소의 특징을 가지게 됨

- 스트림 DSL vs Processor API
  - 스트림 DSL: 상위수준, 필터링, 그룹핑 등의 기능을 제공
  -  Processor API : 조금 더 세밀한 구현이 필요한 경우 로우 레벨의 API 도 제공

----
## 2. 도메인 요구사항과 전략
### 도메인 이벤트 (배달&라이더)
배달 이벤트 : 행위 발생 (배달 생성, 배차, 라이더 픽업, 배달 완료 등
| **상태** | 배달 생성 | ⇒⇒⇒ | 대기 중 | ⇒⇒⇒ | 배차 확정 | ⇒⇒⇒ | 배차 완료 | ⇒⇒⇒ 라이더 픽업 ⇒⇒⇒ | 픽업 완료 | ⇒⇒⇒ 고객에게 전달 ⇒⇒⇒ | 배달 완료 | ⇒⇒⇒ |
|-------|---------|----|-------|----|--------|----|-------|------------------|--------|-----------------|--------|----|
| **발생 데이터** |         | 배달 ID, 배달 상태, **배달 타입, 픽업지/배달지 주소** | | 배달ID, 배달상태, **라이더ID, 배달수단** | | | | | | | | |

라이더 이벤트
| **상태** |  | ⇒⇒운행 시작⇒⇒ | 운행 중 | ⇒⇒운행 종료⇒⇒ | |
|--------|---|-------------|-------|-------------|---|
| **발생 데이터** ||라이더 ID, 라이더 상태, **라이더 타입, 현재 위치**||라이더 ID, 라이더 상태, **현재 위치**||

### 도메인 요구사항
- 기능 요구사항
  1. 배달 + 라이더 이벤트로 특정 라이더의 상황 파악 (ex. ㅇㅇ 라이더는 역삼동에서 배달을 수행중이다.)
  2. 행정동 규모의 세분화된 지역별 현황 집계  (ex. 역삼동에서 배달 중인 라이더는 300명이다.)
- 비기능 요구사항
  - 언제 어디서나 데이터 제공을 보장헐 수 있어야 한다.
 
### (1) 전처리
- 보강) GPS좌표를 행정동으로 변환 ⇒ 라이더 이벤트에 enrich
  - `<라이더 A, GPS(x,y)>` ⇒ `<라이더 A, GPS(x,y), 역삼동>`
- 조인) 라이더 스트림 + 배달 스트림 JOIN ⇒ **라이더 스냅샷 스트림** 생성
  - 배달 스트림 : `<배달 Data, 라이더 A>`
  - 라이더 스트림 : `<라이더 A, GPS(x,y), 역삼동>`
  - 👉 `<라이더 A, 배달 Data, 역삼동 ...>`

### (2) 지역별 데이터 집계
부하가 집중되지 않도록 여러 인스턴스에서 분산 처리 = 스트림 프로세서의 로컬 저장소를 사용하기 때문에 분산 처리가 됨   
= **라이더 스냅샷 스트림을 '행정동' key로 그룹핑 했을 때, 행정동 단위로 집계가 되고 행정동 단위로 상태저장소에 분산 저장 됨**   
🧐 행정동 필드로 Key를 맞춘 후 집계.. 이 때 리파티션이 발생하는 건가?

### (3) 데이터 조회
1. 데이터가 있는 인스턴스로 요청이 들어온 경우 ⇒ 그대로 조회
  *아마 특정 데이터가 있는 파티션을 가진 Pod로 데이터 조회 요청이 들어온 경우의 처리를 말하는듯,,,*
2. 데이터가 없는 인스턴스로 요청이 들어온 경우   
💡 카프카 스트림즈의 경우 <span style="color:red">어떤 데이터가 어느 저장소에 속해있는지의 Meta Data를 가지고 있기 때문에 조회하는데 문제가 되지 않음!</span>
3. 상태 저장소에 접근할 수 없는 경우
   - 상태저장소는 스트림 프로세서 내부에 위치하는데, 이 상태저장소에 접근할 수 없는 경우는 어떤 CASE?
   - **비정상 상태일 때 상태 저장소 접근 불가** (시스템 장애, 배포, 인스턴스 추가/삭제(Scale In,Out) 등)
   - 👉 다운타임 최소화를 위한 백업저장소 구축
   - 집계 처리가 완료된 스트림을 새로운 토픽으로 발행 ⇒ 이 토픽을 Consuming해서 별도의 백업저장소에 저장
     📌 상태 저장소에 접근할 수 없는 경우 백업저장소의 도움을 받아 처리

----
## 3. Trouble Shooting - 이슈와 해결방안
### Issue 1) Producer의 이벤트 메시지 발행 실패
브로커 통신 실패.. 등   
Try 1) 브로커 & 클라이언트 각종 설정값 최적화 (Publisher, Broker, AWS EBS...)   
Try 2) 토픽 & 파티션 검토

### Issue 2) 과도한 토픽 파티션 수
#### 주의할점  
- 토픽의 파티션 수는 <span style="color:red">**한번 늘리면 줄일 수 없음**</span>
- 파티션은 프로커의 파일 시스템 리소스를 사용하기 때문에,매우 큰 파티션은 클러스터 전체 성능의 영향을 줄 수 있다.
- 내부 토픽의 파티션 수는 원본 토픽의 파티션 수를 따른다.
  - 카프카 스트림즈에서는 중간 처리를 위해 생성하는 내부토픽이 있는데 원본 토픽수를 따라가기 때문에 유의해야 한다.

#### 권장
- 적은 수로 시작하고 운영하면서 파티션 수를 늘려가기
- 이미 거대한 파티션이 구성되어 단기간에 조치하기 어렵다면, Bypass 토픽을 생성하서 로직을 처리하는 Consumer 입장에서는 적은 수의 파티션을 감당할 수 있도록 한다.

#### Bad Practice
처음에는 delivery-eventstore라는 하나의 General한 토픽에 모든 데이터를 발행     
⇒ 시스템, 트래픽의 확장에 따라 메시지의 양이 많아지고 파티션을 증설함    
⇒ 컨슈머는 관심있는 데이터 외에 너무 많은 메시지를 수신받고 필터링하게 됨   
⇒ 📌 **토픽을 도메인 단위로 구분하고 적은 수의 파티션으로 처리**    

### Issue 3) 서버 배포와 리밸런싱, Lag의 발생
- 서버 배포시 리밸런싱이 발생하는데, 이 때 특정 스트림 프로세서에 Lag이 과도하게 발생
  - 과도한 Lag을 해결하기 위해 서버 증설은 한계가 있음 👉 파티션 수, 인스턴스 수를 고려해서 적절한 쓰레드 수를 설정하자.
    - 권장 : `파티션 수 = 인스턴스 수 X 쓰레드 수 (num.stream.threads)`
- 리밸런싱
  - 리밸런싱이란? 컨슈머 그룹 내 컨슈머들에게 균등하게 분배하기 위해 파티션 할당을 조정하는 동작
  - 발생 시점 : 파티션 할당 조정이 필요한 경우, 대표적으로 토픽 파티션 수 증설 or 서버 스케일링 or 컨슈머 이슈로 컨슈머 탈락
  - 리밸런싱의 리스크 : 리밸런싱 동안 조정되는 컨슈머의 처리가 중단되므로 고비용 작업이기 때문에, 최대한 불필요한 리밸런싱을 줄여야한다.
    - 리밸런싱 전략 EAGER vs COOPERATIVE
    -  `EAGER` : 모든 컨슈머의 파티션 할당을 끊고 재조정
    -  `COOPERATIVE` :  다운타임을 최소화하는 대신, EAGER보다 여러번, 오래 수행
  - 불필요한 리밸런싱을 줄이기 위한 노력
    - 검토해볼만한 설정 옵션
      - 리밸런싱 전략 (EAGER, COOPERATIVE)
      - 컨슈머 처리속도/처리량 조절을 위한 레코드 폴링 주기/양 (max.poll.interval.ms / max.poll.records)
      - 컨슈머 그룹에서 탈락하지 않기 위한 HeartBeat 기준 (session.timeout.ms / heartbeat.interval.ms)
      - 초반 리밸런싱 대기 시간 (group.initial.rebalance.delay.ms)
    - 파티션/컨슈머 멤버 수가 과하기 많지는 않은지?
    - 결국 근본적으로는 이벤트 로직 최적화
#### Bad Practice
- ISSUE : 서버 배포시 리밸런싱이 발생하는데, 이 때 특정 스트림 프로세서에 Lag이 과도하게 쌓임
- Root Cause : 잘못 작성된 코드로 인한 컨슈머 그룹 내 너무 많은 멤버 수
  - 과한 동시성 부여 Concurrency(
  - 10개의 서로 다른 스트림 프로세서가 동일한 id 값으로 설정됨 (application.id)
  - Spring 인스턴스 수가 8개일 때 총 컨슈머 멤버수 : 32 * 10 * 8 = 2560
  - 심지어 리밸런싱 전략이 **EAGER**여서 모든 파티션-컨슈머를 취소하고 Join/SyncGroup을 수행하는데 Consumer Group Coordinator가 있는 특정 브로커에 요청이 몰리며 부하 발생 ⇒ Lag 발생

### Issue 4) 처리 지연으로 인한 컨슈머 그룹 탈락
- 브로커는 컨슈머들이 메시지들이 잘 처리하는 지 감시하고 잘 처리하지 못하면 컨슈머 그룹에서 탈락 시킴  
- 내부 비즈니스 로직 중 버그로 인한 처리 지연 발생하며 컨슈머 그룹에서 탈락
#### 
- 컨슈머 처리 속도에 따른 적절한 옵션값을 설정하자
  - 컨슈머가 폴링 후, 다음 폴링까지 기다리는 시간 (max.poll.interval.ms)
  - 컨슈머가 한번 폴링할 때, 처리하는 최대 메시지 건수 (max.poll.record)
  - *즉 브로커는 N초안에 M개의 레코드를 처리하기를 기대하기 때문에 컨슈머는 이 기대를 충족시켜주어야 함!*
- 처리 속도와 Consumer Group Leave 로그를 보다 적극적으로 모니터링
- 지연이 발생할만할 부분을 주의하자 (ex. 외부 API 호출 등)

### Issue 5) Disk Usage Critical 알람 발생
- 카프카는 디스크 기반으로 동작함, 확인 결과 예상보다 많은 불필요한 데이터가 리소스를 차지하고 있음
- 

----
## 4. 모니터링과 관리


